{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48e5f38-656e-492f-a992-514b8a771632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 加载模型权重\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = r\"F:\\pythonProject\\distilbert\\distilbert_base_uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 查看模型参数量\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "\n",
    "# hugging face训练的distilbert模型的推理效果\n",
    "import torch\n",
    "text = \"This is a great [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\"\"\"\n",
    "**在函数中的作用就是把后面紧跟着的这个参数，从一个字典的格式，解压成一个个单独的参数。\n",
    "tokenizer的输出是一个包含了input_ids和attention_mask两个key的字典，因此通过**的解压，\n",
    "inputs = {'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]])\n",
    "            , 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "相当于变成了intput_ids=..., attention_mask=...喂给函数model()\n",
    "\"\"\"\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# 1、找到mask对应的词的索引\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "# 2、找到mask词对应的分类特征，0是第一句话（只有一句话）；1是mask_token_index是mask对应的索引；：表示30522个分类特征\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# 3、取出概率前5的结果\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "# 4、将5个最终的预测结果遍历，decode([token])是将预测的索引转换为词\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ee116e-b61f-4b07-9346-8ddcf7f1f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用imdb影评数据集微调distilbert模型\"\"\"\n",
    "# 1、加载imdb数据集\n",
    "from datasets import load_from_disk\n",
    "path = r'F:\\pythonProject\\datasets\\stanfordnlp\\imdb'  \n",
    "imdb_dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ee124e-9389-407d-a2ca-b13603af4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b9bcf4362747a18dc6a070454c9986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79625a8ab4d64ddf8b68c61c43379ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e20bebb8f49e4afea479e277491a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2、加载分词器\n",
    "model_checkpoint = r\"F:\\pythonProject\\distilbert\\distilbert_base_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        # 计算每一个文本的长度（word_ids）\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# map操作是对数据集的每个样本都执行该操作，batched表示批量处理\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9307827-abb1-44a4-9b1d-4a014eb1680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对整个数据集进行分块操作\n",
    "def group_texts(examples):\n",
    "    chunk_size = 128\n",
    "    # 拼接到一起\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # 计算长度\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # //就相当于咱们不要最后多余的了\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # 切分\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # 完型填空会用到标签的，就是原来被mask掉的文本，所以标签是和原来的id是一样的\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9f6843-0f42-4cd8-a007-d63ed583c9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efd4922dab6489988044f7a7ff69a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ee4374583c4ad799ff0eff03612b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1085968fd7046c88432bed6abb0e226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用map方法执行group_texts()这个函数\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb888f5-396d-4744-bd76-7405c4e47230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、数据封装，将数据封装为一个batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "# mlm_probability参数定义了在输入序列中每个单词被选择为掩盖的概率,默认值0.15\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e12599-a9e7-4a31-9e57-335b16543525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 采样数据集，并分割数据集\n",
    "train_size = 10000\n",
    "test_size = int(0.1 * train_size)\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e860fd-58f7-49f8-89c1-49c05ba8efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定训练参数\n",
    "from transformers import TrainingArguments\n",
    "batch_size = 64\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./FinetuneModel/distilbert_finetuned_imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d3df77-513d-4948-91ab-5cd7b2775201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 1:00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.690700</td>\n",
       "      <td>2.531834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=157, training_loss=2.689167912598628, metrics={'train_runtime': 3666.3116, 'train_samples_per_second': 2.728, 'train_steps_per_second': 0.043, 'total_flos': 331402890240000.0, 'train_loss': 2.689167912598628, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae250c49-a175-4f97-919c-00342cfd4b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 01:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 12.68\n"
     ]
    }
   ],
   "source": [
    "# 评估指标：困惑度\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "# 困惑度就是交叉熵的指数形式，简单的说就是当在选择mask位置什么词合适时，平均挑了多少个词才能答对。\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d46a56-0ebd-4df1-89d6-927ba20190d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great idea.'\n",
      "'>>> This is a great deal.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great job.'\n",
      "'>>> This is a great one.'\n"
     ]
    }
   ],
   "source": [
    "# 使用imdb影评数据集微调distilbert后的模型推理\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = r\"F:\\pythonProject\\distilbert\\distilbert_base_uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(r\"F:\\pythonProject\\imdb_distilbert_finetuned\\checkpoint-157\")\n",
    "\n",
    "# 新模型的效果\n",
    "import torch\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda4a0b-9b3a-4857-b376-d0e896d91f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
