{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9971edc8-6c07-47a3-9430-f310e7c352d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at F:\\pythonProject\\google_bert\\bert_base_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1、加载模型和预训练权重\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = r\"F:\\pythonProject\\google_bert\\bert_base_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058f7fdf-0c4a-4f8b-bb71-e1eea122913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c96e0-0265-442f-af36-85cbfdeae167",
   "metadata": {},
   "source": [
    "看到这么一大串的warning出现，不要怕，这个warning正是我们希望看到的。\r\n",
    "\r\n",
    "为啥会出现这个warning呢，因为我们加载的预训练权重是bert-based-uncased，而使用的骨架是AutoModelForSequenceClassification，前者是没有在下游任务上微调过的，所以用带有下游任务Head的骨架去加载，会随机初始化这个Head。这些在warning中也说的很明白。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d86e1c5-46a2-4101-b8b2-88494ca48ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、加载数据集\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = r\"F:\\pythonProject\\datasets\\glue\\mrpc\"\n",
    "raw_datasets = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ae7b69-53ef-4549-a8a7-7d3f527dd588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3、查看整个数据集结构\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c65cbf-9ba6-4d74-af73-2acb40503b07",
   "metadata": {},
   "source": [
    "load_dataset出来的是一个DatasetDict对象，它包含了train，validation，test三个属性。可以通过key来直接查询，得到对应的train、valid和test数据集。\r\n",
    "\r\n",
    "这里的train，valid，test都是Dataset类型，有 features和num_rows两个属性。还可以直接通过下标来查询对应的本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dcf0070-31b2-49e9-ae2b-8a88e6b44185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1查看单个样本数据结构\n",
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c63021-aa17-4ca6-8796-774ea1e21889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2查看单个样本的特征\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab38f8a-ac6b-402e-b840-9af25256da29",
   "metadata": {},
   "source": [
    "Dataset的features可以理解为一张表的columns，Dataset甚至可以看做一个pandas的dataframe，二者的使用很类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1714e073-b963-4a55-835e-82b60eade8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd1629a0e6437d84dfff2fe25f2e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4、数据集预处理\n",
    "def tokenize_function(sample):\n",
    "    # 这里可以添加多种操作，不光是tokenize\n",
    "    # 这个函数处理的对象，就是Dataset这种数据类型，通过features中的字段来选择要处理的数据\n",
    "    return tokenizer(sample['sentence1'], sample['sentence2'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceac368-7269-45d9-bf13-de03451c882c",
   "metadata": {},
   "source": [
    "官方推荐的做法是通过Dataset.map方法，来调用一个分词方法，实现批量化的分词。注意到，在这个tokenize_function中，没有使用padding，因为如果使用了padding之后，就会全局统一做一个maxlen进行padding，这样无论在tokenize还是模型的训练上都不够高效。实际上，我们是故意先不进行padding的，因为我们想在划分batch的时候再进行padding，这样可以避免出现很多有一堆padding的序列，从而可以显著节省我们的训练时间。这样就需要用到**DataCollatorWithPadding，来进行动态padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b7a808-a711-4fbb-a0b1-b112756299ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 动态padding，collator数据\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45285a18-ee80-47de-b9df-c26b3a95f7cc",
   "metadata": {},
   "source": [
    "注意，我们需要使用tokenizer来初始化这个DataCollatorWithPadding，因为需要tokenizer来告知具体的padding token是啥，\n",
    "以及padding的方式是在左边还是右边（不同的预训练模型，使用的padding token以及方式可能不同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88765ca2-85d0-47d1-9154-191bff051c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# 5、数据导入Pytorch的Dataloader前，执行分词处理后样本数据的列表名称\n",
    "print(tokenized_datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d37922-2750-4ecf-a910-c199e4f60921",
   "metadata": {},
   "source": [
    "huggingface中datasets准备了三个方法：remove_columns, rename_column, set_format，方便为pytorch的Dataloader做准备:\n",
    "\n",
    "* 去除‘sentence1’，‘sentence2’等\n",
    "* 把数据转换成pytorch tensors\n",
    "* 修改列名 label 为 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2ac1f3-c15a-411d-8896-5d22be1742d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2','idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label','labels')  \n",
    "tokenized_datasets.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a0fdfc-d3ba-41cd-be53-ef6515df7606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6446fb1-8d9d-4bca-a8d0-4f4f8251887b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 74]),\n",
       " 'token_type_ids': torch.Size([8, 74]),\n",
       " 'attention_mask': torch.Size([8, 74])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1定义pytorch dataloaders\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# 通过这里的dataloader，每个batch的seq_len可能不同\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator)  \n",
    "eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# 查看一下train_dataloader的元素长啥样\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5358ee8-2195-4912-8018-16601c8dd47c",
   "metadata": {},
   "source": [
    "按道理说，Huggingface这边提供Transformer模型就已经够了，具体的训练、优化，应该交给pytorch了吧。\n",
    "但鉴于Transformer训练时，最常用的优化器就是AdamW，这里Huggingface也直接在transformers库中加入\n",
    "了AdamW这个优化器，还贴心地配备了lr_scheduler，方便我们直接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4936f52-3482-4dc6-b81a-072a068d6a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6、定义 optimizer 和 learning rate scheduler\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)  # num of batches * num of epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,  # scheduler是针对optimizer的lr的\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6c92a-d977-4dbd-a5c3-6b5b2fc7385e",
   "metadata": {},
   "source": [
    "编写pytorch training loops，思路如下：\n",
    "\r\n",
    "* .for每一个epoch\n",
    "* *.从dataloader里取出一个个batch\n",
    "* 3.把batch喂给model（先把batch都移动到对应的device上）\n",
    "* 4.拿出loss，进行反向传播backward\n",
    "* 5.分别把optimizer和scheduler都更新一个stepl一个step\r\n",
    "最后别忘了每次更新都要清空grad，即对optimizer进行zero_grad()操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126db947-9532-41b5-9461-878855480376",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e137f6-5a3a-4604-91b3-684dcec5a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7、Pytorch training loops\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # 要在GPU上训练，需要把数据集都移动到GPU上：\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3605321-491e-4f60-a829-a3fc3d83de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8、Evaluation\n",
    "from datasets import load_metric\n",
    "\n",
    "metric= load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():  # evaluation的时候不需要算梯度\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # 由于dataloader是每次输出一个batch，因此我们要等着把所有batch都添加进来，再进行计算\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
