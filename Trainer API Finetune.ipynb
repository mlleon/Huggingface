{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9971edc8-6c07-47a3-9430-f310e7c352d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at F:\\pythonProject\\google_bert\\bert_base_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1、加载模型和预训练权重\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = r\"F:\\pythonProject\\google_bert\\bert_base_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058f7fdf-0c4a-4f8b-bb71-e1eea122913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c96e0-0265-442f-af36-85cbfdeae167",
   "metadata": {},
   "source": [
    "看到这么一大串的warning出现，不要怕，这个warning正是我们希望看到的。\r\n",
    "\r\n",
    "为啥会出现这个warning呢，因为我们加载的预训练权重是bert-based-uncased，而使用的骨架是AutoModelForSequenceClassification，前者是没有在下游任务上微调过的，所以用带有下游任务Head的骨架去加载，会随机初始化这个Head。这些在warning中也说的很明白。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d86e1c5-46a2-4101-b8b2-88494ca48ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2、加载数据集\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = r\"F:\\pythonProject\\datasets\\glue\\mrpc\"\n",
    "raw_datasets = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ae7b69-53ef-4549-a8a7-7d3f527dd588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3、查看整个数据集结构\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c65cbf-9ba6-4d74-af73-2acb40503b07",
   "metadata": {},
   "source": [
    "load_dataset出来的是一个DatasetDict对象，它包含了train，validation，test三个属性。可以通过key来直接查询，得到对应的train、valid和test数据集。\r\n",
    "\r\n",
    "这里的train，valid，test都是Dataset类型，有 features和num_rows两个属性。还可以直接通过下标来查询对应的本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dcf0070-31b2-49e9-ae2b-8a88e6b44185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1查看单个样本数据结构\n",
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c63021-aa17-4ca6-8796-774ea1e21889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2查看单个样本的特征\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab38f8a-ac6b-402e-b840-9af25256da29",
   "metadata": {},
   "source": [
    "Dataset的features可以理解为一张表的columns，Dataset甚至可以看做一个pandas的dataframe，二者的使用很类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1714e073-b963-4a55-835e-82b60eade8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4、数据集预处理\n",
    "def tokenize_function(sample):\n",
    "    # 这里可以添加多种操作，不光是tokenize\n",
    "    # 这个函数处理的对象，就是Dataset这种数据类型，通过features中的字段来选择要处理的数据\n",
    "    return tokenizer(sample['sentence1'], sample['sentence2'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceac368-7269-45d9-bf13-de03451c882c",
   "metadata": {},
   "source": [
    "官方推荐的做法是通过Dataset.map方法，来调用一个分词方法，实现批量化的分词。注意到，在这个tokenize_function中，没有使用padding，因为如果使用了padding之后，就会全局统一做一个maxlen进行padding，这样无论在tokenize还是模型的训练上都不够高效。实际上，我们是故意先不进行padding的，因为我们想在划分batch的时候再进行padding，这样可以避免出现很多有一堆padding的序列，从而可以显著节省我们的训练时间。这样就需要用到**DataCollatorWithPadding，来进行动态padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b7a808-a711-4fbb-a0b1-b112756299ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='F:\\pythonProject\\google_bert\\bert_base_uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1 动态padding，collator数据\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45285a18-ee80-47de-b9df-c26b3a95f7cc",
   "metadata": {},
   "source": [
    "注意，我们需要使用tokenizer来初始化这个DataCollatorWithPadding，因为需要tokenizer来告知具体的padding token是啥，\n",
    "以及padding的方式是在左边还是右边（不同的预训练模型，使用的padding token以及方式可能不同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88765ca2-85d0-47d1-9154-191bff051c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5、实例化trainer对象\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir='test_trainer') # 指定输出文件夹，没有会自动创建\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,  # 在定义了tokenizer之后，其实这里的data_collator就不用再写了，会自动根据tokenizer创建\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4dd1f-ca6f-47e6-858b-49822aaee28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1启动训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077920d-f273-4166-947a-bd01fa5c3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 \n",
    "trainer.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a0f3c-9096-4b88-af46-a6d4b82aae3f",
   "metadata": {},
   "source": [
    "trainer.predict()函数处理的结果是一个named_tuple（一种可以直接通过key来取值的tuple），类似一个字典，包含三个属性：predictions, label_ids, metrics\n",
    "* predictions实际上就是logits\r",
    "* \n",
    "label_ids不是预测出来的id，而是数据集中自带的ground truth的标签，因此如果输入的数据集中没给标签，这里也不会输\n",
    "* metrics，也是只有输入的数据集中提供了label_ids才会输出metrics，包括loss之类的指标参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0831ac4-156d-4a65-9466-980dcece3d2f",
   "metadata": {},
   "source": [
    "其中metrics中还可以包含我们自定义的字段，我们需要在定义Trainer的时候给定compute_metrics参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c925460-6bf3-4e86-a15a-d1b7a52da502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
